# MonkeyTest üêµüß™

AI-powered browser testing with Browser Use - TypeScript test runner for natural language browser automation.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue.svg)](https://www.typescriptlang.org/)
[![pnpm](https://img.shields.io/badge/pnpm-8.15-orange.svg)](https://pnpm.io/)

## Overview

MonkeyTest is a TypeScript-based test runner that lets you write browser tests in plain English using markdown files. Powered by [Browser Use](https://browser-use.com) AI agents, your tests are executed automatically with no need to write complex Selenium or Playwright scripts.

## Features

- ü§ñ **AI-Powered**: Write tests in natural language, let AI handle the browser automation
- üìù **Markdown Tests**: Simple, readable test cases in markdown format with frontmatter
- üé® **Beautiful Output**: Colored console output with chalk for better readability
- üìä **Rich Reports**: Detailed JSON results with test outcomes and artifacts
- üíæ **Artifacts**: Capture screenshots and generated files
- üéØ **Flexible**: Support for multiple LLM models and custom configurations
- ‚ö° **Fast**: Built with TypeScript and modern async/await patterns
- üîß **Type-Safe**: Full TypeScript support with strict typing
- üîç **Git Diff Testing**: Auto-generate tests from code changes using LLM analysis
- üöÄ **GitHub Actions Integration**: Automated test generation and execution in CI/CD

## Quick Start

### Prerequisites

- Node.js 18+ 
- pnpm 8+ (install with `npm install -g pnpm`)
- Browser Use API key from [Browser Use Cloud](https://cloud.browser-use.com)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/monkey-test.git
cd monkey-test

# Install dependencies
pnpm install

# Build the project
pnpm build
```

### Configuration

Set your Browser Use API key as an environment variable:

```bash
export BROWSER_USE_API_KEY="your-api-key-here"
```

## Usage Modes

MonkeyTest supports two modes of operation:

### 1. Standard Mode - Run Existing Tests

Run pre-written markdown test cases from your test directory:

```bash
# Run all tests in the tests directory
monkey-test

# Or use npm/pnpm scripts
pnpm test
```

### 2. Diff-Based Test Generation (NEW!)

Automatically generate and run tests based on git commit changes:

```bash
# Generate tests from changes since a specific commit
monkey-test --from-commit main

# Generate tests from last 3 commits
monkey-test --from-commit HEAD~3

# Generate tests only (don't execute them)
monkey-test --from-commit main --generate-only
```

#### How Diff-Based Testing Works

1. **Get Git Diff**: Compares your current code with the specified commit
2. **LLM Analysis**: Sends the diff to GPT-4 to analyze changes
3. **Test Generation**: LLM generates browser test cases in XML format
4. **Convert to Markdown**: Converts generated tests to markdown format
5. **Execute Tests**: Runs the generated tests using Browser Use
6. **Save Artifacts**: Saves diff, LLM response, and test results
7. **GitHub Actions**: Displays results in PR comments and workflow summaries

#### Required Environment Variables for Diff Mode

```bash
export OPENAI_API_KEY="your-openai-api-key"
export BROWSER_USE_API_KEY="your-browser-use-api-key"
```

#### Optional Configuration

```bash
# LLM model for test generation (default: gpt-4-turbo-preview)
export TEST_GENERATION_MODEL="gpt-4-turbo-preview"

# Max test cases to generate (default: 10)
export MAX_TEST_CASES=10

# Max diff size in characters (default: 100000)
export MAX_DIFF_SIZE=100000

# Artifact directory (default: artifacts)
export ARTIFACT_DIR="./artifacts"

# Max concurrent tests (default: 3)
export MAX_CONCURRENCY=3

# Test timeout in seconds (default: 300)
export TIMEOUT=300
```

## GitHub Actions Integration

MonkeyTest includes built-in GitHub Actions support for automated testing on PRs and commits.

### Setup GitHub Actions Workflow

1. Add secrets to your repository:
   - `BROWSER_USE_API_KEY`: Your Browser Use API key
   - `OPENAI_API_KEY`: Your OpenAI API key

2. Use the provided workflow file at `.github/workflows/diff-test.yml`

3. The workflow will automatically:
   - Generate tests from PR changes
   - Execute the tests
   - Upload artifacts (diffs, generated tests, results)
   - Comment on PRs with test results
   - Display summary in workflow page

### Workflow Features

- **Pull Request Testing**: Automatically tests changes in PRs
- **Artifact Storage**: Saves all generated tests and results for 30 days
- **PR Comments**: Posts test results directly on pull requests
- **Test Preview**: Optional job to generate tests without executing
- **Manual Trigger**: Run workflow manually with custom commit reference

### Example Workflow Output

The GitHub Actions workflow provides:

1. **Step Summary**: Formatted markdown table with test results
2. **Artifacts**: 
   - Generated test cases (`.md` files)
   - Git diff used for generation
   - Raw LLM response
   - Test execution results (JSON + Markdown)
   - Screenshots and outputs
3. **PR Comments**: Automatic comment with pass/fail status
4. **Annotations**: Error annotations for failed tests

## Command Line Options

```bash
monkey-test [OPTIONS]

OPTIONS:
  --from-commit <ref>   Generate tests from git diff (commit reference)
  --generate-only       Only generate tests, don't execute them
  --help                Show help message

EXAMPLES:
  # Standard mode
  monkey-test

  # Generate from main branch
  monkey-test --from-commit main

  # Generate from specific commit
  monkey-test --from-commit abc123

  # Preview generated tests only
  monkey-test --from-commit HEAD~1 --generate-only
```

## Environment Variables Reference

### Required (Standard Mode)
- `BROWSER_USE_API_KEY`: API key for Browser Use Cloud

### Required (Diff Mode)
- `BROWSER_USE_API_KEY`: API key for Browser Use Cloud
- `OPENAI_API_KEY`: API key for OpenAI

### Optional
- `TEST_DIRECTORY`: Test files directory (default: `tests`)
- `LLM_MODEL`: Model for test execution (default: `browser-use-llm`)
- `TEST_GENERATION_MODEL`: Model for test generation (default: `gpt-4-turbo-preview`)
- `TIMEOUT`: Test timeout in seconds (default: `300`)
- `MAX_CONCURRENCY`: Max concurrent tests (default: `3`)
- `MAX_TEST_CASES`: Max tests to generate (default: `10`)
- `MAX_DIFF_SIZE`: Max diff size in chars (default: `100000`)
- `ARTIFACT_DIR`: Artifact output directory (default: `artifacts`)
- `OUTPUT_DIR`: Test output directory (default: `browser-use-outputs`)
- `FAIL_ON_ERROR`: Exit with error on failure (default: `true`)
- `SAVE_OUTPUTS`: Save test outputs (default: `true`)

## Examples

### Example 1: Test PR Changes

```bash
# Get the base branch of your PR
BASE_COMMIT=$(git merge-base HEAD origin/main)

# Generate and run tests
OPENAI_API_KEY="sk-..." BROWSER_USE_API_KEY="bu-..." \
  monkey-test --from-commit $BASE_COMMIT
```

### Example 2: Preview Generated Tests

```bash
# Generate tests without executing
OPENAI_API_KEY="sk-..." \
  monkey-test --from-commit main --generate-only

# View generated tests
ls -la .monkey-test-generated/
```

### Example 3: Custom Test Generation

```bash
# Generate more tests with larger diff size
MAX_TEST_CASES=20 MAX_DIFF_SIZE=200000 \
  OPENAI_API_KEY="sk-..." BROWSER_USE_API_KEY="bu-..." \
  monkey-test --from-commit HEAD~5
```

## Artifacts Structure

When using diff-based testing, the following artifacts are created:

```
artifacts/
‚îú‚îÄ‚îÄ diff-2024-01-15T10-30-00-000Z.txt          # Git diff
‚îú‚îÄ‚îÄ llm-response-2024-01-15T10-30-00-000Z.txt  # Raw LLM response
‚îú‚îÄ‚îÄ test-results-2024-01-15T10-30-00-000Z.json # Test results (JSON)
‚îî‚îÄ‚îÄ test-results-2024-01-15T10-30-00-000Z.md   # Test results (Markdown)

.monkey-test-generated/
‚îú‚îÄ‚îÄ 1-test-login-functionality.md
‚îú‚îÄ‚îÄ 2-test-dashboard-rendering.md
‚îî‚îÄ‚îÄ 3-test-api-integration.md

browser-use-outputs/
‚îú‚îÄ‚îÄ test-login-functionality/
‚îÇ   ‚îú‚îÄ‚îÄ screenshot-1.png
‚îÇ   ‚îî‚îÄ‚îÄ output.txt
‚îî‚îÄ‚îÄ test-dashboard-rendering/
    ‚îî‚îÄ‚îÄ screenshot-1.png
```

## Troubleshooting

### Common Issues

**Error: "OPENAI_API_KEY environment variable is required"**
- Set your OpenAI API key: `export OPENAI_API_KEY="sk-..."`

**Error: "Not a git repository"**
- Run monkey-test from within a git repository
- Ensure `.git` directory exists

**Error: "Invalid commit reference"**
- Verify the commit exists: `git log --oneline`
- Use valid references: `main`, `HEAD~1`, commit SHA, etc.

**Error: "No changes found between commits"**
- The diff is empty - no changes detected
- Check if you're comparing identical commits

**LLM generates invalid XML**
- Try running again - LLMs can occasionally produce malformed output
- Check your API key and quota limits
- Try a different model with `TEST_GENERATION_MODEL`

### Debug Mode

Enable verbose logging:

```bash
export BROWSER_USE_API_KEY="bu_..."
```

Or create a `.env` file:

```env
BROWSER_USE_API_KEY=bu_...
TEST_DIRECTORY=tests
LLM_MODEL=browser-use-llm
TIMEOUT=300
SAVE_OUTPUTS=true
FAIL_ON_ERROR=true
```

### Create Your First Test

Create a test file in the `tests` directory:

**tests/example-test.md**:
```markdown
---
name: "Homepage Load Test"
description: "Verify homepage loads correctly"
timeout: 60
---

# Task

Navigate to https://example.com and verify:
1. The page title is "Example Domain"
2. The main heading says "Example Domain"
3. There is a "More information" link

Return "PASS" if all elements are present, otherwise "FAIL" with details.
```

### Run Tests

```bash
# Run tests
pnpm test

# Or run directly with tsx (development)
pnpm dev

# Or run the built version
pnpm start
```

## Project Structure

```
monkey-test/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts           # Main entry point
‚îÇ   ‚îú‚îÄ‚îÄ types.ts           # TypeScript type definitions
‚îÇ   ‚îú‚îÄ‚îÄ config.ts          # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ test-parser.ts     # Markdown test parser
‚îÇ   ‚îú‚îÄ‚îÄ test-executor.ts   # Test execution logic
‚îÇ   ‚îú‚îÄ‚îÄ reporter.ts        # Report generation and display
‚îÇ   ‚îî‚îÄ‚îÄ utils.ts           # Utility functions
‚îú‚îÄ‚îÄ tests/                 # Your test markdown files
‚îú‚îÄ‚îÄ browser-use-outputs/   # Test artifacts (generated)
‚îú‚îÄ‚îÄ dist/                  # Compiled JavaScript (generated)
‚îú‚îÄ‚îÄ package.json           # Dependencies and scripts
‚îú‚îÄ‚îÄ tsconfig.json          # TypeScript configuration
‚îî‚îÄ‚îÄ README.md              # This file
```

## Test Case Format

Test cases are written in Markdown with YAML frontmatter:

```markdown
---
name: "Test Name"
description: "Optional description"
timeout: 120
llm_model: "browser-use-llm"
input_files: []
expected_output: "Optional expected output"
---

# Task

Your test instructions in plain English...

1. Navigate to a URL
2. Interact with elements
3. Verify conditions
4. Return results
```

### Frontmatter Options

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `name` | string | No | Filename | Test case name |
| `description` | string | No | "" | Test description |
| `timeout` | number | No | 300 | Timeout in seconds |
| `llm_model` | string | No | browser-use-llm | LLM model to use |
| `input_files` | array | No | [] | Input file IDs |
| `expected_output` | string | No | - | Expected output for validation |

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `BROWSER_USE_API_KEY` | (required) | Your Browser Use API key |
| `TEST_DIRECTORY` | `tests` | Directory containing test files |
| `LLM_MODEL` | `browser-use-llm` | Default LLM model |
| `TIMEOUT` | `300` | Default timeout in seconds |
| `SAVE_OUTPUTS` | `true` | Save output files |
| `FAIL_ON_ERROR` | `true` | Exit with error code on test failure |
| `OUTPUT_DIR` | `browser-use-outputs` | Directory for output files |
| `MAX_CONCURRENCY` | `3` | Maximum number of concurrent test sessions |

## Scripts

```json
{
  "build": "tsc",                    // Compile TypeScript to JavaScript
  "dev": "tsx src/index.ts",         // Run in development mode
  "start": "node dist/index.js",     // Run compiled version
  "test": "node dist/index.js",      // Run tests (after build)
  "clean": "rm -rf dist",            // Clean build artifacts
  "typecheck": "tsc --noEmit"        // Type check without compiling
}
```

## Example Tests

### Simple Page Check

```markdown
---
name: "Page Load Test"
---

# Task

Go to https://example.com and verify the page loads successfully.
Return "PASS" if successful.
```

### Form Testing

```markdown
---
name: "Contact Form Test"
timeout: 120
---

# Task

1. Navigate to https://example.com/contact
2. Fill in the form:
   - Name: "Test User"
   - Email: "test@example.com"
   - Message: "This is a test message"
3. Submit the form
4. Verify the success message appears
5. Return the confirmation message text
```

### Data Extraction

```markdown
---
name: "Extract Headlines"
llm_model: "browser-use-llm"
---

# Task

Go to https://news.ycombinator.com and extract the top 10 post titles and URLs.
Return as JSON array with "title" and "url" fields.
```

## Output

### Console Output

The test runner provides colored, formatted console output:

```
üöÄ Browser Use Test Runner
================================================================================

üîß Initializing Browser Use client...
‚úÖ Browser Use client initialized successfully

üìÅ Found 3 test file(s) in 'tests'

üéØ Starting test execution...
üìÅ Test directory: tests
‚öôÔ∏è  LLM Model: browser-use-llm
‚è±Ô∏è  Timeout: 300s per test
üíæ Save outputs: true

================================================================================
üß™ Running test: Homepage Load Test
üìÑ File: tests/example-test.md
üìù Description: Verify homepage loads correctly
================================================================================
üöÄ Creating Browser Use task...
üìã Task instructions: Navigate to https://example.com and verify...
‚úÖ Task created: task_abc123
‚è≥ Waiting for task completion (timeout: 60s)...
üìä Status: running
üìä Status: finished
‚úÖ Test PASSED in 12.34s
üì§ Output: PASS - All elements verified

================================================================================
üìä TEST SUMMARY
================================================================================
Total Tests:    3
‚úÖ Passed:      3
‚ùå Failed:      0
‚ö†Ô∏è  Errors:      0
Success Rate:   100.0%
================================================================================

üìã Individual Test Results:
  ‚úÖ Homepage Load Test (12.34s)
  ‚úÖ Contact Form Test (45.67s)
  ‚úÖ Extract Headlines (23.45s)

üíæ Results saved to: test-results.json
‚úÖ All tests passed!
```

### JSON Output

Results are saved to `test-results.json`:

```json
{
  "summary": {
    "total": 3,
    "passed": 3,
    "failed": 0,
    "errors": 0,
    "successRate": "100.0%"
  },
  "results": [
    {
      "name": "Homepage Load Test",
      "filePath": "tests/example-test.md",
      "status": "passed",
      "output": "PASS - All elements verified",
      "duration": 12.34,
      "taskId": "task_abc123",
      "outputFiles": []
    }
  ]
}
```

## GitHub Actions Integration

The test runner supports GitHub Actions through the `GITHUB_OUTPUT` environment variable:

```yaml
name: Browser Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: pnpm/action-setup@v2
        with:
          version: 8
      
      - uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: 'pnpm'
      
      - name: Install dependencies
        run: pnpm install
      
      - name: Build
        run: pnpm build
      
      - name: Run tests
        id: tests
        env:
          BROWSER_USE_API_KEY: ${{ secrets.BROWSER_USE_API_KEY }}
        run: pnpm test
      
      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            test-results.json
            browser-use-outputs/
      
      - name: Check results
        run: |
          echo "Total: ${{ steps.tests.outputs.total-tests }}"
          echo "Passed: ${{ steps.tests.outputs.passed-tests }}"
          echo "Failed: ${{ steps.tests.outputs.failed-tests }}"
```

## API Reference

### BrowserUseClient

```typescript
import { BrowserUseClient } from "browser-use-sdk";

const client = new BrowserUseClient({
  apiKey: "bu_...",
});

// Create a task
const task = await client.tasks.createTask({
  task: "Navigate to example.com",
  llm: "browser-use-llm",
  inputFiles: [], // optional
});

// Stream progress
for await (const update of task.stream()) {
  console.log(update.status);
}

// Get final result
const result = await task.complete();
console.log(result.output);
console.log(result.outputFiles);
```

## Tips for Writing Effective Tests

### ‚úÖ Do's

- **Be specific**: "Click the blue 'Submit' button on the contact form"
- **Set boundaries**: "Extract the first 10 items from the list"
- **Include URLs**: Always specify the full URL to navigate to
- **Define output format**: "Return as JSON with fields: name, email, status"
- **Add context**: Explain login credentials or prerequisites
- **Use timeouts**: Set realistic timeouts based on page complexity

### ‚ùå Don'ts

- **Too vague**: "Test the website" - what specifically?
- **Too broad**: "Check all functionality" - be more specific
- **Missing details**: "Login to the app" - with what credentials?
- **No success criteria**: How do you know if the test passed?
- **Unrealistic timeouts**: Give complex tests enough time

## Troubleshooting

### API Key Not Found

```
‚ùå Error: BROWSER_USE_API_KEY environment variable is not set
```

**Solution**: Export the environment variable or add it to your `.env` file.

### Test Directory Not Found

```
‚ùå Error: Test directory 'tests' does not exist
```

**Solution**: Create the test directory or set `TEST_DIRECTORY` to an existing directory.

### Tests Timeout

**Solutions**:
- Increase the timeout in test frontmatter or `TIMEOUT` env var
- Simplify test instructions
- Break complex tests into smaller ones
- Check if the target website is slow

### TypeScript Compilation Errors

```bash
# Check for type errors
pnpm typecheck

# Clean and rebuild
pnpm clean
pnpm build
```

### Module Resolution Issues

Make sure you're using `.js` extensions in imports:

```typescript
// ‚úÖ Correct
import { Config } from "./types.js";

// ‚ùå Wrong
import { Config } from "./types";
```

## Development

### Setup Development Environment

```bash
# Install dependencies
pnpm install

# Run in development mode (with auto-reload)
pnpm dev

# Type check
pnpm typecheck

# Build
pnpm build
```

### Project Architecture

- **`index.ts`**: Main entry point and test runner orchestration
- **`config.ts`**: Environment variable loading and validation
- **`types.ts`**: TypeScript interfaces and types
- **`test-parser.ts`**: Markdown parsing with frontmatter support
- **`test-executor.ts`**: Test execution with Browser Use SDK
- **`reporter.ts`**: Console and JSON report generation
- **`utils.ts`**: Shared utility functions

### Adding New Features

1. Add types to `types.ts`
2. Implement logic in appropriate module
3. Update tests
4. Update documentation

## Contributing

Contributions are welcome! Please feel free to:

- Report bugs and issues
- Suggest new features
- Submit pull requests
- Improve documentation
- Share example test cases

## Resources

- [Browser Use Documentation](https://docs.cloud.browser-use.com)
- [Browser Use Cloud Dashboard](https://cloud.browser-use.com)
- [Browser Use SDK](https://github.com/browser-use/browser-use-sdk)
- [TypeScript Documentation](https://www.typescriptlang.org/docs/)
- [pnpm Documentation](https://pnpm.io/)

## License

MIT License - See LICENSE file for details

## Support

- üí¨ [GitHub Issues](https://github.com/yourusername/monkey-test/issues)
- üìñ [Documentation](https://github.com/yourusername/monkey-test)
- üåê [Browser Use Community](https://browser-use.com)

---

**Happy Testing!** üêµüß™

Made with ‚ù§Ô∏è using [Browser Use](https://browser-use.com)